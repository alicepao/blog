[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Blog",
    "section": "",
    "text": "Data Wrangling Project Showcase: Praxis Data\n\n\n\n\n\n\nportfolio\n\n\nr\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\nAlice Pao\n\n\n\n\n\n\n\n\n\n\n\n\nWindoes Function in PySpark\n\n\n\n\n\n\ntutorial\n\n\ncode\n\n\npyspark\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\nAlice Pao\n\n\n\n\n\n\n\n\n\n\n\n\nNested Columns in Pyspark\n\n\n\n\n\n\ntutorial\n\n\ncode\n\n\npolars\n\n\n\n\n\n\n\n\n\nFeb 24, 2024\n\n\nAlice Pao\n\n\n\n\n\n\n\n\n\n\n\n\nHow to pull data from census.gov?\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nFeb 11, 2024\n\n\nAlice Pao\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "How to pull data from census.gov?",
    "section": "",
    "text": "US Census has provided a lot of great data resources on topics like American population, places, and economy. These data can be used for data analysis projects and even training data for machine learning models. In this post, I’ll walk through the steps to pulling data from its API."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! This is Alice. I am studying data science at Brigham Young University-Idaho. I love story-telling, especially using data to find insights and solve problems.\nI am an aspiring data analyst. I specialize in using SQL for data extraction, R for data wrangling and data visualization, Excel for data anaysis, and PowerBI and Tableau for buidling dashoboards and report."
  },
  {
    "objectID": "posts/welcome/index.html#request-access-key",
    "href": "posts/welcome/index.html#request-access-key",
    "title": "How to pull data from census.gov?",
    "section": "Request Access Key",
    "text": "Request Access Key\nFirst, we need to request an access key from this website. Fill out your information and once submitted, an email will be sent shortly to your email with an unique key. This key can be reused so remember to save it somewhere else.\n\nAfter getting your key, we are ready to access data from the API."
  },
  {
    "objectID": "posts/welcome/index.html#find-data-table",
    "href": "posts/welcome/index.html#find-data-table",
    "title": "How to pull data from census.gov?",
    "section": "Find Data Table",
    "text": "Find Data Table\nThere are a couple of ways to find the data you want. You can go to the census data website to browse all provided tables there. Use the search bar to find what you want or if you already know what table you are looking for, you can type it in directly. For example, I want to collect the household income in 2020 from the American Community Survey. I then find the corresponding table number and head to this database page.\nHint: Table name should be the 6-digit Number. If you are not sure, you can also find it from the Notes tab.\n\nOn census’s database page, use ctrl+f to find your table. Right click variables and use your table name to find the data. In my case, it is B19001.\n\nIn this household income table, different codes represent different income gropus; therefore, I want to pull all of them so I can have the entire data set."
  },
  {
    "objectID": "posts/welcome/index.html#connect-to-census-api",
    "href": "posts/welcome/index.html#connect-to-census-api",
    "title": "How to pull data from census.gov?",
    "section": "Connect to Census API",
    "text": "Connect to Census API\nHere comes the most important part, connecting to and download the data! First, make sure you download and import the census library.\n\n# from census import Census\n# import polars as pl\n# c = Census(\"YOUR ACCESS KEY\")\n\n# # Total Numbers of Household for total household income\n# total = c.acs5.state_county_blockgroup(('NAME', 'B19001_001E'), '16', '*', Census.ALL, year=2020)\n\n# # Print the retrieved data\n# #for row in data:\n# #    print(row)\n# df_total = pd.DataFrame(total)\n# # Rename columns\n# df_total = df_total.rename(columns={'NAME': 'Geography Group', 'B19001_001E': 'Total Numbers of Household', 'state': 'Idaho', 'county':'County', 'tract':'Tract', 'block group':'Block Group'})\n\n# # Display the DataFrame\n# df_total\n\nThis code block pulls the B19001_001E column, which is the total number of household from all income groups. Since I want to have data for other income groups as well, I wrote a function (with the help of ChatGPT of course) to handle this.\n\n# def get_income_group_data(year, income_min, income_max, state_code='16', county_code='*'):\n#     # Retrieve data for the specified income group\n#     group_data = c.acs5.state_county_blockgroup(('NAME', f'B19001_{income_min:03d}E'), state_code, county_code, Census.ALL, year=year)\n    \n#     # Convert data to DataFrame\n#     df_group = pl.DataFrame(group_data)\n    \n#     # Rename columns for better readability\n#     df_group = df_group.rename({'NAME': 'geography_group', f'B19001_{income_min:03d}E': 'total_numbers_of_household', 'state': 'idaho', 'block group':'block_group'})\n    \n#     return df_group\n\n\n# df_total = get_income_group_data(2020, 1, 2) # Income Group: total\n# df_10k_less = get_income_group_data(2020, 2, 3) # Income Group: less than $10,000\n# df_10k = get_income_group_data(2020, 3, 4) # Income Group: $10,000 - $14,999\n# df_15k = get_income_group_data(2020, 4, 5) #Income Group: $15,000 - $19,999\n# df_20k = get_income_group_data(2020, 5, 6) # Income Group: $20,000 - $24,999\n# df_25k = get_income_group_data(2020, 6, 7) # Income Group: $25,000 - $29,999\n# df_30k = get_income_group_data(2020, 7, 8) # Income Group: $30,000 - $44,999\n# df_35k = get_income_group_data(2020, 8, 9) # Income Group: $35,000 - $39,999\n# df_40k = get_income_group_data(2020, 9, 10) # Income Group: $40,000 - $44,999\n# df_45k = get_income_group_data(2020, 10, 11)  # Income Group: $45,000 - $49,999, Idaho\n# df_50k = get_income_group_data(2020, 11, 12) # Income Group: $50,000 - $59,999, Idaho \n# df_60k = get_income_group_data(2020, 12, 13) # Income Group: $60,000 - $74,999\n# df_75k = get_income_group_data(2020, 13, 14) # Income Group: $75,000 - $99,999\n# df_100k = get_income_group_data(2020, 14, 15) # Income Group: $100,000 - $124,999\n# df_125k = get_income_group_data(2020, 15, 16) # Income Group: $125,000 - $149,999\n# df_150k = get_income_group_data(2020, 16, 17) # Income Group: $150,000 - $199,999\n# df_200k = get_income_group_data(2020, 17, 18) # Income Group: $200,000+"
  },
  {
    "objectID": "posts/welcome/index.html#combine-all-datasets",
    "href": "posts/welcome/index.html#combine-all-datasets",
    "title": "How to pull data from census.gov?",
    "section": "Combine All Datasets",
    "text": "Combine All Datasets\nAfter pulling all data from each income group, I created a new column to lable the groups before appending them. I use polars for data wrangling. It is a fairly new library similar to pandas but it is more efficient. I’ll introduce it more in the future. Here, I used with_columns() to create a new column. pl.lit() represents a literal value.\n\n# create a new column in each data set to indicate its income group\n# df_10k_less = df_10k_less.with_columns(\n#     income_group = pl.lit('10,000 less')\n# )\n# df_total = df_total.with_columns(\n#     income_group = pl.lit('total')\n# )\n# df_10k = df_10k.with_columns(\n#     income_group = pl.lit('10,000-14,999')\n# )\n# df_15k = df_15k.with_columns(\n#     income_group = pl.lit('15,000-19,999')\n# )\n\n# df_20k = df_20k.with_columns(\n#     income_group = pl.lit('20,000-24,999')\n# )\n# df_25k = df_25k.with_columns(\n#     income_group = pl.lit('25,000-29,999')\n# )\n# df_30k = df_30k.with_columns(\n#     income_group = pl.lit('30,000-34,999')\n# )\n# df_35k = df_35k.with_columns(\n#     income_group = pl.lit('35,000-39,999')\n# )\n# df_40k = df_40k.with_columns(\n#     income_group = pl.lit('40,000-44,999')\n# )\n# df_45k = df_45k.with_columns(\n#     income_group = pl.lit('45,000-49,999')\n# )\n# df_50k = df_50k.with_columns(\n#     income_group = pl.lit('50,000-59,999')\n# )\n# df_60k = df_60k.with_columns(\n#     income_group = pl.lit('60,000-74,999')\n# )\n# df_75k = df_75k.with_columns(\n#     income_group = pl.lit('75,000-99,999')\n# )\n# df_100k = df_100k.with_columns(\n#     income_group = pl.lit('100,000-124,999')\n# )\n# df_125k = df_125k.with_columns(\n#     income_group = pl.lit('125,000-149,999')\n# )\n# df_150k = df_100k.with_columns(\n#     income_group = pl.lit('150,000-199,999')\n# )\n# df_200k = df_100k.with_columns(\n#     income_group = pl.lit('200,000+')\n# )\n\nAfter creating columns for each data set, I use concat() to append them all into a new data set. concat() is used to combine all DataFrames, LazyFrames, and Series. For the how parameter, the options include: vertical’, ‘vertical_relaxed’, ‘diagonal’, ‘diagonal_relaxed’, ‘horizontal’, ‘align’.\n\n# # combine all rows together\n# df_all_income_group = pl.concat(\n#     [\n#         df_10k_less,\n#         df_10k,\n#         df_15k, \n#         df_20k, \n#         df_25k, \n#         df_30k, \n#         df_35k, \n#         df_40k, \n#         df_45k, \n#         df_50k, \n#         df_60k, \n#         df_75k, \n#         df_100k, \n#         df_125k, \n#         df_150k, \n#         df_200k, \n#         df_total\n#     ],\n#     how=\"vertical\",\n# )"
  },
  {
    "objectID": "posts/welcome/index.html#conclusion",
    "href": "posts/welcome/index.html#conclusion",
    "title": "How to pull data from census.gov?",
    "section": "Conclusion",
    "text": "Conclusion\nI hope this tutorial helps you figure out how to connect to the census api and download the data you need. After getting the data, you’ll need to clean or transform them to a better format but this should give you a great start."
  },
  {
    "objectID": "posts/pull_data_from_census/index.html",
    "href": "posts/pull_data_from_census/index.html",
    "title": "How to pull data from census.gov?",
    "section": "",
    "text": "US Census has provided a lot of great data resources on topics like American population, places, and economy. These data can be used for data analysis projects and even training data for machine learning models. In this post, I’ll walk through the steps to pulling data from its API."
  },
  {
    "objectID": "posts/pull_data_from_census/index.html#request-access-key",
    "href": "posts/pull_data_from_census/index.html#request-access-key",
    "title": "How to pull data from census.gov?",
    "section": "Request Access Key",
    "text": "Request Access Key\nFirst, we need to request an access key from this website. Fill out your information and once submitted, an email will be sent shortly to your email with an unique key. This key can be reused so remember to save it somewhere else.\n\nAfter getting your key, we are ready to access data from the API."
  },
  {
    "objectID": "posts/pull_data_from_census/index.html#find-data-table",
    "href": "posts/pull_data_from_census/index.html#find-data-table",
    "title": "How to pull data from census.gov?",
    "section": "Find Data Table",
    "text": "Find Data Table\nThere are a couple of ways to find the data you want. You can go to the census data website to browse all provided tables there. Use the search bar to find what you want or if you already know what table you are looking for, you can type it in directly. For example, I want to collect the household income in 2020 from the American Community Survey. I then find the corresponding table number and head to this database page.\nHint: Table name should be the 6-digit Number. If you are not sure, you can also find it from the Notes tab.\n\nOn census’s database page, use ctrl+f to find your table. Right click variables and use your table name to find the data. In my case, it is B19001.\n\nIn this household income table, different codes represent different income gropus; therefore, I want to pull all of them so I can have the entire data set."
  },
  {
    "objectID": "posts/pull_data_from_census/index.html#connect-to-census-api",
    "href": "posts/pull_data_from_census/index.html#connect-to-census-api",
    "title": "How to pull data from census.gov?",
    "section": "Connect to Census API",
    "text": "Connect to Census API\nHere comes the most important part, connecting to and download the data! First, make sure you download and import the census library.\n\n# from census import Census\n# import polars as pl\n# c = Census(\"YOUR ACCESS KEY\")\n\n# # Total Numbers of Household for total household income\n# total = c.acs5.state_county_blockgroup(('NAME', 'B19001_001E'), '16', '*', Census.ALL, year=2020)\n\n# # Print the retrieved data\n# #for row in data:\n# #    print(row)\n# df_total = pd.DataFrame(total)\n# # Rename columns\n# df_total = df_total.rename(columns={'NAME': 'Geography Group', 'B19001_001E': 'Total Numbers of Household', 'state': 'Idaho', 'county':'County', 'tract':'Tract', 'block group':'Block Group'})\n\n# # Display the DataFrame\n# df_total\n\nThis code block pulls the B19001_001E column, which is the total number of household from all income groups. Since I want to have data for other income groups as well, I wrote a function (with the help of ChatGPT of course) to handle this.\n\n# def get_income_group_data(year, income_min, income_max, state_code='16', county_code='*'):\n#     # Retrieve data for the specified income group\n#     group_data = c.acs5.state_county_blockgroup(('NAME', f'B19001_{income_min:03d}E'), state_code, county_code, Census.ALL, year=year)\n    \n#     # Convert data to DataFrame\n#     df_group = pl.DataFrame(group_data)\n    \n#     # Rename columns for better readability\n#     df_group = df_group.rename({'NAME': 'geography_group', f'B19001_{income_min:03d}E': 'total_numbers_of_household', 'state': 'idaho', 'block group':'block_group'})\n    \n#     return df_group\n\n\n# df_total = get_income_group_data(2020, 1, 2) # Income Group: total\n# df_10k_less = get_income_group_data(2020, 2, 3) # Income Group: less than $10,000\n# df_10k = get_income_group_data(2020, 3, 4) # Income Group: $10,000 - $14,999\n# df_15k = get_income_group_data(2020, 4, 5) #Income Group: $15,000 - $19,999\n# df_20k = get_income_group_data(2020, 5, 6) # Income Group: $20,000 - $24,999\n# df_25k = get_income_group_data(2020, 6, 7) # Income Group: $25,000 - $29,999\n# df_30k = get_income_group_data(2020, 7, 8) # Income Group: $30,000 - $44,999\n# df_35k = get_income_group_data(2020, 8, 9) # Income Group: $35,000 - $39,999\n# df_40k = get_income_group_data(2020, 9, 10) # Income Group: $40,000 - $44,999\n# df_45k = get_income_group_data(2020, 10, 11)  # Income Group: $45,000 - $49,999, Idaho\n# df_50k = get_income_group_data(2020, 11, 12) # Income Group: $50,000 - $59,999, Idaho \n# df_60k = get_income_group_data(2020, 12, 13) # Income Group: $60,000 - $74,999\n# df_75k = get_income_group_data(2020, 13, 14) # Income Group: $75,000 - $99,999\n# df_100k = get_income_group_data(2020, 14, 15) # Income Group: $100,000 - $124,999\n# df_125k = get_income_group_data(2020, 15, 16) # Income Group: $125,000 - $149,999\n# df_150k = get_income_group_data(2020, 16, 17) # Income Group: $150,000 - $199,999\n# df_200k = get_income_group_data(2020, 17, 18) # Income Group: $200,000+"
  },
  {
    "objectID": "posts/pull_data_from_census/index.html#combine-all-datasets",
    "href": "posts/pull_data_from_census/index.html#combine-all-datasets",
    "title": "How to pull data from census.gov?",
    "section": "Combine All Datasets",
    "text": "Combine All Datasets\nAfter pulling all data from each income group, I created a new column to lable the groups before appending them. I use polars for data wrangling. It is a fairly new library similar to pandas but it is more efficient. I’ll introduce it more in the future. Here, I used with_columns() to create a new column. pl.lit() represents a literal value.\n\n# create a new column in each data set to indicate its income group\n# df_10k_less = df_10k_less.with_columns(\n#     income_group = pl.lit('10,000 less')\n# )\n# df_total = df_total.with_columns(\n#     income_group = pl.lit('total')\n# )\n# df_10k = df_10k.with_columns(\n#     income_group = pl.lit('10,000-14,999')\n# )\n# df_15k = df_15k.with_columns(\n#     income_group = pl.lit('15,000-19,999')\n# )\n\n# df_20k = df_20k.with_columns(\n#     income_group = pl.lit('20,000-24,999')\n# )\n# df_25k = df_25k.with_columns(\n#     income_group = pl.lit('25,000-29,999')\n# )\n# df_30k = df_30k.with_columns(\n#     income_group = pl.lit('30,000-34,999')\n# )\n# df_35k = df_35k.with_columns(\n#     income_group = pl.lit('35,000-39,999')\n# )\n# df_40k = df_40k.with_columns(\n#     income_group = pl.lit('40,000-44,999')\n# )\n# df_45k = df_45k.with_columns(\n#     income_group = pl.lit('45,000-49,999')\n# )\n# df_50k = df_50k.with_columns(\n#     income_group = pl.lit('50,000-59,999')\n# )\n# df_60k = df_60k.with_columns(\n#     income_group = pl.lit('60,000-74,999')\n# )\n# df_75k = df_75k.with_columns(\n#     income_group = pl.lit('75,000-99,999')\n# )\n# df_100k = df_100k.with_columns(\n#     income_group = pl.lit('100,000-124,999')\n# )\n# df_125k = df_125k.with_columns(\n#     income_group = pl.lit('125,000-149,999')\n# )\n# df_150k = df_100k.with_columns(\n#     income_group = pl.lit('150,000-199,999')\n# )\n# df_200k = df_100k.with_columns(\n#     income_group = pl.lit('200,000+')\n# )\n\nAfter creating columns for each data set, I use concat() to append them all into a new data set. concat() is used to combine all DataFrames, LazyFrames, and Series. For the how parameter, the options include: vertical’, ‘vertical_relaxed’, ‘diagonal’, ‘diagonal_relaxed’, ‘horizontal’, ‘align’.\n\n# # combine all rows together\n# df_all_income_group = pl.concat(\n#     [\n#         df_10k_less,\n#         df_10k,\n#         df_15k, \n#         df_20k, \n#         df_25k, \n#         df_30k, \n#         df_35k, \n#         df_40k, \n#         df_45k, \n#         df_50k, \n#         df_60k, \n#         df_75k, \n#         df_100k, \n#         df_125k, \n#         df_150k, \n#         df_200k, \n#         df_total\n#     ],\n#     how=\"vertical\",\n# )"
  },
  {
    "objectID": "posts/pull_data_from_census/index.html#conclusion",
    "href": "posts/pull_data_from_census/index.html#conclusion",
    "title": "How to pull data from census.gov?",
    "section": "Conclusion",
    "text": "Conclusion\nI hope this tutorial helps you figure out how to connect to the census api and download the data you need. After getting the data, you’ll need to clean or transform them to a better format but this should give you a great start."
  },
  {
    "objectID": "posts/nested_columns_in_pyspark/index.html",
    "href": "posts/nested_columns_in_pyspark/index.html",
    "title": "Nested Columns in Pyspark",
    "section": "",
    "text": "When dealing with great volume of data which we often refer to as “big data”, it is common to have nested columns. They not only help with data format but also save a lot of memory. One of the most popular file formats that use nested columns is Parquet. Therefore, before jumping into nested columns, I want to spend some time explaining what Parquet is."
  },
  {
    "objectID": "posts/nested_columns_in_pyspark/index.html#what-is-parquet",
    "href": "posts/nested_columns_in_pyspark/index.html#what-is-parquet",
    "title": "Nested Columns in Pyspark",
    "section": "What is Parquet?",
    "text": "What is Parquet?\nApache Parquet, mostly called as Parquet, is an open source file format that saves column-based data, created by databricks. It provides efficient data storeage and retrival. It is used to handle complex data structure and it compresses and decompresses data in a more efficient way compared to other traditional file formats like CSV.\nParquet can store data including images, tables, videos, and documents. One of its benefits is its ability to skip data, meaning when data is read in by queries, instead of reading the entire table, it only grabs the specific columns where the queries specifiy. This saves a lot of data processing time.\nBelow is taken from databricks. It compares Parquet with CVS. We can see that using Parquet file format is both cheaper and faster. Therefore, I highly recommend data science professionals familarize themselves with Parquet!"
  },
  {
    "objectID": "posts/nested_columns_in_pyspark/index.html#what-is-nested-columns",
    "href": "posts/nested_columns_in_pyspark/index.html#what-is-nested-columns",
    "title": "Nested Columns in Pyspark",
    "section": "What is Nested Columns?",
    "text": "What is Nested Columns?\nNow, let’s talk about about nested columns. Just like the words suggest, a nested column is a column that has multiple comlumns nested in it. (I swear this is not a tongue twister!) You may ask, why do we need it? Well, having nested columns give us more flexibility as we can add additional columns into existing ones, which gives more control when we are dealing with complex data.\nHere is an example of normal columns:\n\nThis one takes the first name, middle name, and last name columns and nest them into a name column.\n\nBy doing so, we make a cleaner and more organized data structure and also save some storage space and data retrieval time."
  },
  {
    "objectID": "posts/nested_columns_in_pyspark/index.html#different-types-of-nested-columns-in-pyspark",
    "href": "posts/nested_columns_in_pyspark/index.html#different-types-of-nested-columns-in-pyspark",
    "title": "Nested Columns in Pyspark",
    "section": "Different Types of Nested Columns in Pyspark",
    "text": "Different Types of Nested Columns in Pyspark\nThere are different types of nested columns we can utilize in Pyspark.\n\nArrayType It is an array of data type. Users can store a collection of same type of elements.\nMapType It is a map of key value pairs in a DataFrame. Users can store key-value mappings. It is similar to a dictionary in python.\nStructType It is a list or tuple value type in Python. It stores a collection of StructField."
  },
  {
    "objectID": "posts/nested_columns_in_pyspark/index.html#how-to-create-nested-columns",
    "href": "posts/nested_columns_in_pyspark/index.html#how-to-create-nested-columns",
    "title": "Nested Columns in Pyspark",
    "section": "How to Create Nested Columns?",
    "text": "How to Create Nested Columns?\nBefore creating any of these datatypes, we need to import the following function from pyspark.\n\n\nCode\n# from pyspark.sql.types import *\n\n\nHere are the parameters for each of the three data types:\n\nArrayType:\n\n\nelementType: Defines the DataType for all elements in the array\ncontainsNull: This is an optional parameter. It declares whether the array can have null values or not.\n\n\n\nCode\n# Array_Data = [(3500, 5000, 8000, 2500, 4000)]\n\n\n# Array_Schema = ArrayType(IntegerType())\n# array = spark.createDataFrame(data = Array_Data, schema = Array_Schema)   \n# array.printSchema()   \n# display(array)    \n\n\nThe Result: \n\nMapType:\n\n\nkeyType: Defines the DataType for the keys in the map.\nvalueType: Defines the DataType for the values in the map.\nvalueContainsNull: This is an optional parameter. It declares whether the map can have null values or not.\n\n\n\n\nCode\n# map_data = [('Katy Wellen', {'junior graphic designer':3500}), ('Samuel Komo', {'junior data analyst':5000}), ('Luke Hitch', {'senior data analyst':8000}), ('Mike Nelson', {'office assistant':2500}), ('Cathy Reid', {'project manager':4000})]\n# map_schema = StructType([\n#     StructField('full name', StringType(), True),\n#     StructField('position & salary', MapType(StringType(),IntegerType()),True)\n# ])\n# map = spark.createDataFrame(data = map_data, schema = map_schema) \n# map.printSchema() \n# display(map)\n\n\n\nStructType: Before talking about StructType, we need to first understand StructField. StructField is an object that consists of name(a string), dataType, and the nullable. Here’s an example:\n\n\n\nCode\n# StructField(\"first_name\", StringType(), True)\n\n\nStructType is created by calling the StructField. For example:\n\n\nCode\n# Structure_Data = [((\"Katy\",\"\",\"Wellen\"),\"11111\",\"F\",3500),    \n#     ((\"Samuel\",\"Komo\",\"\"),\"22222\",\"M\",5000),  \n#     ((\"Luke\",\"\",\"Hitch\"),\"33333\",\"M\",8000),   \n#     ((\"Mike\",\"Smith\",\"Nelson\"),\"44444\",\"M\",2500), \n#     ((\"Cathy\",\"Reid\",\"Brown\"),\"\",\"F\",4000)    \n#   ]   \n\n# Structure_Schema = StructType([   \n#         StructField('name', StructType([  \n#              StructField('first_name', StringType(), True),   \n#              StructField('middle_name', StringType(), True),  \n#              StructField('last_name', StringType(), True) \n#              ])), \n#          StructField('id', StringType(), True),   \n#          StructField('gender', StringType(), True),   \n#          StructField('salary', IntegerType(), True)   \n#          ])\n\n# dataframe = spark.createDataFrame(data = Structure_Data, schema = Structure_Schema)\n# dataframe.printSchema()   \n# display(dataframe)"
  },
  {
    "objectID": "posts/nested_columns_in_pyspark/index.html#how-to-unnest",
    "href": "posts/nested_columns_in_pyspark/index.html#how-to-unnest",
    "title": "Nested Columns in Pyspark",
    "section": "How to “Unnest”?",
    "text": "How to “Unnest”?\nThe easieset way is to use explode function. For example, let’s use our map variable from the previous instance.\n\n\nCode\n# map_explode = map.select('full name', F.explode(map['position & salary'])).withColumnRenamed(\n#     'key', 'position').withColumnRenamed('value', 'salary')\n\n# display(map_explode)\n\n\nHere’s a snapshot before unnesting: \nHere’s after:"
  },
  {
    "objectID": "posts/nested_columns_in_pyspark/index.html#conclusion",
    "href": "posts/nested_columns_in_pyspark/index.html#conclusion",
    "title": "Nested Columns in Pyspark",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "posts/data_wrangling_project/index.html",
    "href": "posts/data_wrangling_project/index.html",
    "title": "Data Wrangling Project Showcase: Praxis Data",
    "section": "",
    "text": "This project is designed to show students’ praxis performance for those majoring in teacher’s education department. This is only a part of the project. The end goal is to evalutate if a student’s praxis score can predict wheather he or she becomes a successful teacher."
  },
  {
    "objectID": "posts/data_wrangling_project/index.html#data-source",
    "href": "posts/data_wrangling_project/index.html#data-source",
    "title": "Data Wrangling Project Showcase: Praxis Data",
    "section": "Data Source",
    "text": "Data Source\nThe data was collected from a higher-education instition in Idaho. Subjects are students from the institute’s teacher education department. All students’ identities are shielded."
  },
  {
    "objectID": "posts/data_wrangling_project/index.html#data-definition",
    "href": "posts/data_wrangling_project/index.html#data-definition",
    "title": "Data Wrangling Project Showcase: Praxis Data",
    "section": "Data Definition",
    "text": "Data Definition\n\nstudent_id: a unique identifier for students majoring in education department.\ntest_name: different praxis tests\ntest_code: a unique identifier for praxis tests\nfirst_score: the score a student got when he or she first took the praxis test. If a student takes multiple praxis tests (different test codes), then he or she will have multiple first scores.\nlast_score: the score a student got for his or her last praxis test. Again, if a student takes multiple praxis tests (different test codes), there will be multiple last scores.\nn_attempt: indicates the total attempts a student has for each praxis test he or she takes. For example, if a student took Middle School English Language Arts test 17 times, his n_attempt will be 17.\nn_attempt_pass: indicates the number of tests a student passed for a specific praxis test. For example, if a student took Middle School English Language Arts test 17 times and didn’t pass any of it, then his n_attempt_pass will be 0.\never_pass: indicates if a student eventually passed the test or not.\nstate_passing_score: indicates the minimum score a student is required to pass a certain praxis test. This threshold is determined by Idaho’s education department.\nfirst_test_date: the first test date a student took his or her very first test.\nlast_test_date: the last test data a student took his or her very last test.\ntotal_tests: the total number count for each test.\naverage_first_score: the average test score for all students first attempt.\nfirst_standard_deviation: the standard deviation for all students’ first attempt.\nfirst_z_score: the z score for all students’ first attempt.\naverage_last_score: the average test score for all students’ last attempt.\nlast_standard_deviation: the standard deviation for all students’ last attempt.\nlast_z_score: the z score for all students’ z score.\nfirst_score_cutoff: points away from the state’s passing standard for a student’s first attempt\nlast_score_cutoff: points away from the state’s passing standard for a student’s last attempt\naverage_z_score: the average z score taken from the first and last attempt z score"
  },
  {
    "objectID": "posts/data_wrangling_project/index.html#data-summary",
    "href": "posts/data_wrangling_project/index.html#data-summary",
    "title": "Data Wrangling Project Showcase: Praxis Data",
    "section": "Data Summary",
    "text": "Data Summary\nBefore diving in, here’s a table showing all our collected data.\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(lubridate)\nlibrary(Hmisc)\nlibrary(pander)\nlibrary(DT)\n\n\n\n\nCode\npath &lt;- \"/Users/alice/Library/Mobile Documents/com~apple~CloudDocs/Portfolio/Data Science Blog/alice_in_dataland/posts/data_wrangling_project/final_praxis.csv\"\npraxis &lt;- read_csv(path)\n# show praxis data table???\npraxis %&gt;% DT::datatable()\n\n\n\n\n\n\nThe following table is a quick data summary for our numerical columns.\n\n\nCode\npraxis %&gt;%\n  select(first_score, last_score, n_attempt, n_attempt_pass, ever_pass, state_passing_score)%&gt;%\n  summary %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfirst_score\nlast_score\nn_attempt\nn_attempt_pass\never_pass\nstate_passing_score\n\n\n\n\n\nMin. : 1.0\nMin. : 4.0\nMin. : 1.000\nMin. :0.0000\nMin. :0.0000\nMin. :129.0\n\n\n\n1st Qu.:161.0\n1st Qu.:164.0\n1st Qu.: 1.000\n1st Qu.:1.0000\n1st Qu.:1.0000\n1st Qu.:154.0\n\n\n\nMedian :172.0\nMedian :172.0\nMedian : 1.000\nMedian :1.0000\nMedian :1.0000\nMedian :157.0\n\n\n\nMean :173.2\nMean :175.3\nMean : 1.271\nMean :0.9545\nMean :0.9496\nMean :157.8\n\n\n\n3rd Qu.:181.0\n3rd Qu.:181.0\n3rd Qu.: 1.000\n3rd Qu.:1.0000\n3rd Qu.:1.0000\n3rd Qu.:159.0\n\n\n\nMax. :780.0\nMax. :780.0\nMax. :17.000\nMax. :4.0000\nMax. :1.0000\nMax. :630.0\n\n\n\nNA\nNA\nNA\nNA’s :132\nNA’s :132\nNA’s :127\n\n\n\n\n\nIn total, we have 4,810 students in this data set and other than the Elem Ed tests (MS Mathematics Subtest, MS Science Subtest, MS Social Studies Subtest, and MS Reading & Language Arts Subtest), the most took 5 tests are the following: * English Language Arts: Content Knowledge * Principles of Learn & Teaching: Grades K-6 * Middle School English Language Arts * World and US History: Content Knowledge * Special Ed: Preschool/Early Childhood."
  },
  {
    "objectID": "posts/data_wrangling_project/index.html#data-transformation-process",
    "href": "posts/data_wrangling_project/index.html#data-transformation-process",
    "title": "Data Wrangling Project Showcase: Praxis Data",
    "section": "Data Transformation Process",
    "text": "Data Transformation Process"
  },
  {
    "objectID": "posts/data_wrangling_project/index.html#graphs",
    "href": "posts/data_wrangling_project/index.html#graphs",
    "title": "Data Wrangling Project Showcase: Praxis Data",
    "section": "Graphs",
    "text": "Graphs\nThe following graph shows the most taken praxis test other than the 4 Elem Ed tests.\n\nEnglish Language Arts: Content Knowledge\nPrinciples of Learn & Teaching: Grades K-6\nMiddle School English Language Arts\nWorld and US History: Content Knowledge\nSpecial Ed: Preschool/Early Childhood\n\n\n\nCode\n# Most Taken Praxis Test\npraxis %&gt;% \n  group_by(test_name) %&gt;% \n  summarise(total_count = n()) %&gt;% \n  arrange(desc(total_count)) %&gt;% \n  filter(total_count&lt;1600) %&gt;%\n  top_n(5, total_count) %&gt;% \nggplot(aes(x = reorder(test_name, -total_count), y = total_count))+\n  geom_bar(stat = \"identity\", fill = \"skyblue\")+\n  theme_classic()+\n  geom_text(aes(label = total_count))+\n  theme(axis.text.x = element_text(angle = 90))+\n  labs(x = \"Test Name\", y = \"Total Count\", title = \"Most Taken Praxis Test\")+\n  theme(plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\nThis graph shows the tests having the highest retake rate. One thing to be aware of is that I filter to tests that have more than 100 attempts from students.\n\nMiddle School English Language Arts\nMathematics: Content Knowledge\nMiddle School Science\nElem Ed: MS Mathematics Subtest\nSpanish: World Language\n\n\n\nCode\n# Most Retake% Praxis Test\npraxis|&gt;\n  group_by(test_name) |&gt;\n  summarise(\n    n_attempt = n(),\n    second_attempt = sum(!is.na(`2`)),\n    prop = round(second_attempt / n_attempt, 2)*100) |&gt;\n  top_n(17, prop) |&gt;\n  arrange(desc(prop)) |&gt;\n  filter(n_attempt &gt; 100) %&gt;% \n  ggplot(aes(x = reorder(test_name, -prop), y = prop))+\n  geom_bar(stat = \"identity\", fill = \"skyblue\")+\n  theme_classic()+\n  geom_text(aes(label= prop))+\n  theme(axis.text.x = element_text(angle = 90))+\n  labs(x = \"Test Name\", y = \"Retake Rate(%)\", title = \"Praxis Tests with the Highest Retake Rate\")+\n  theme(plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\nThe last graph shows the Lowest Pass Rate. Likewise, I also filtered this down to tests that have more than 100 attempts.\n\nElem Ed: CKT Social Studies Subtest\nElem Ed: CKT REading & Language Arts Subtest\nSpanish: World Language\nMiddle School English Language Arts\nElem Ed: MS Mathematics Subtest\n\n\n\nCode\n# Low Pass%\npraxis|&gt;\n  group_by(test_name) |&gt;\n  summarise(\n    n_attempt = sum(n_attempt),\n    pass = sum(ever_pass),\n    prop = round(pass / n_attempt, 2)*100) |&gt;\n  filter(prop!=100) |&gt;\n  filter(n_attempt &gt; 100) |&gt;\n  top_n(-5, prop) |&gt;\n  ggplot(aes(x = reorder(test_name, -prop), y = prop))+\n  geom_bar(stat = \"identity\", fill = \"skyblue\")+\n  theme_classic()+\n  geom_text(aes(label= prop))+\n  theme(axis.text.x = element_text(angle = 90))+\n  labs(x = \"Test Name\", y = \"Pass Rate (%)\", title = \"Lowest Pass Rate\")+\n  theme(plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\nLastly, the table below shows the total attempts by students for each test and its pass rate. It is ordered from the most taken to the least tests and the darker the shade for the pass rate cell, the higher the pass rate.\n\n\nCode\npass_rate &lt;- praxis|&gt;\n  group_by(test_name) |&gt;\n  summarise(\n    n_attempt = sum(n_attempt),\n    pass = sum(ever_pass),\n    rate = round(pass / n_attempt, 2)*100) |&gt;\n  arrange(desc(n_attempt), desc(rate)) |&gt;\n  filter(n_attempt &gt; 100)\n\nn &lt;- c(seq(1, 100))\n\nbrks &lt;- quantile(n, probs = seq(.05, .95, .05), na.rm = TRUE)\nclrs &lt;- round(seq(255, 40, length.out = length(brks) + 1), 0) %&gt;%\n  {paste0(\"rgb(255,\", ., \",\", ., \")\")}\ndatatable(pass_rate) %&gt;% formatStyle('rate', backgroundColor = styleInterval(brks, clrs))"
  },
  {
    "objectID": "posts/data_wrangling_project/index.html#data-wrangling-process",
    "href": "posts/data_wrangling_project/index.html#data-wrangling-process",
    "title": "Data Wrangling Project Showcase: Praxis Data",
    "section": "Data Wrangling Process",
    "text": "Data Wrangling Process\nWhen I first got my hands on this data set, each row represents all tests taken from all students. Since each student can take multiple praxis test, there are a lot of data. However, as a team, we decided to only narrow down to the first and last test score per student per test.\nThere are 4 main parts of data transformation:\n\nOnly remain the first and last test per student per test\nCalculate the # of attempts, # of passed attempts, if a student ever passed\nGrab the date for first and last test\nJoin all data frames together\n\nThe following are my code for data wrangling:\n\n\nCode\npath &lt;- \"Praxis Data.xlsx\"\npraxis &lt;- read_excel(path)\n\n# Get First & Last Test Score\nfirst_last &lt;- praxis |&gt;\n  unique() |&gt;\n  group_by(StudentID, TestName) |&gt;\n  filter(TestDate == min(TestDate) | TestDate == max(TestDate)) |&gt;\n  arrange(TestDate) |&gt;\n  mutate(order = 1:n()) |&gt;\n  ungroup() |&gt;\n  select(StudentID, TestName, TestCode, Score, order) |&gt;\n  pivot_wider(names_from = order, values_from = Score) |&gt;\n  mutate(\n    last_score = ifelse(is.na(`2`), `1`, `2`))\n\nfirst_last &lt;- first_last %&gt;% \n  rename(first_score = `1`)\n\n\n# TestCode, Ever Pass, # Attempt, Attempt Passed, Sate Standard\nattempt_test &lt;- praxis %&gt;% \n  unique() %&gt;% \n  group_by(StudentID, TestName) %&gt;%\n  mutate(\n    n_attempt = n(), \n    n_attempt_pass = sum(ifelse(PassNotPass == \"Passed\", 1, 0)), \n    ever_pass = ifelse(n_attempt_pass == 0, 0, 1)\n  ) %&gt;% \n  ungroup() %&gt;% \n  select(StudentID, TestCode, n_attempt, n_attempt_pass, ever_pass, StatePassingScore) %&gt;% unique()\n\n# join first_last & attempt_test\nfinal &lt;- left_join(first_last, attempt_test, by = c(\"StudentID\", \"TestCode\")) \n\n# Date First & Last \ndate &lt;- praxis |&gt;\n  unique() |&gt;\n  group_by(StudentID, TestName) |&gt;\n  filter(TestDate == min(TestDate) | TestDate == max(TestDate)) |&gt;\n  arrange(TestDate) |&gt;\n  mutate(order = 1:n()) |&gt;\n  ungroup() |&gt;\n  select(StudentID, TestName, TestCode, TestDate, order) |&gt;\n  pivot_wider(names_from = order, values_from = TestDate) |&gt;\n  mutate(\n  last_test = ifelse(is.na(`2`), `1`, `2`),\n   last_test_date = format(as.POSIXlt(last_test, origin = lubridate::origin),format='%Y-%m-%d'), \n   last_test_date = as.Date(last_test_date) + days(1)) |&gt;\n  select(StudentID, TestCode, `1`, last_test_date) |&gt;\n  rename(first_test_date = `1`, student_id = StudentID, test_code = TestCode)\n\nfinal &lt;- final %&gt;% \n  rename(student_id = StudentID, test_name = TestName, test_code = TestCode, state_passing_score = StatePassingScore)\nfinal &lt;- left_join(final, date, by = c(\"student_id\", \"test_code\"))\n\n# Z Score\nfinal &lt;- final %&gt;%\n  # Z score\n  group_by(test_code) %&gt;%\n  mutate(\n    # counting the total for each test\n    total_tests = n(),\n    # average score for each test\n    average_first_score = sum(first_score)/total_tests,\n    first_standard_deviation = sd(first_score),\n    first_z_score = round((first_score-average_first_score)/first_standard_deviation, 2), \n    average_last_score = sum(last_score)/total_tests, \n    last_standard_deviation = sd(last_score), \n    last_z_score = round((last_score-average_last_score)/last_standard_deviation, 2), \n    first_score_cutoff = first_score-state_passing_score, \n    last_score_cutoff = last_score-state_passing_score\n    ) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    average_z_score = (first_z_score+last_z_score)/2\n  )"
  }
]